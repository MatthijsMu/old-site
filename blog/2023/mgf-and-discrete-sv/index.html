<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Moment-Generating Functions for some discrete s.v.'s | Matthijs Muis</title> <meta name="author" content="Matthijs Muis"> <meta name="description" content="Let's do the derivations!"> <meta name="keywords" content="Undergraduate student Mathematics, Computer Science, Radboud University Nijmegen, Portfolio, Teaching, Blog"> <meta property="og:site_name" content="Matthijs Muis"> <meta property="og:type" content="article"> <meta property="og:title" content="Matthijs Muis | Moment-Generating Functions for some discrete s.v.'s"> <meta property="og:url" content="https://matthijsmu.github.io/blog/2023/mgf-and-discrete-sv/"> <meta property="og:description" content="Let's do the derivations!"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Moment-Generating Functions for some discrete s.v.'s"> <meta name="twitter:description" content="Let's do the derivations!"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://matthijsmu.github.io/blog/2023/mgf-and-discrete-sv/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Matthijs </span>Muis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/blog/">Blog</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/repositories/">Repositories</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Teaching</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/teaching/">University Courses</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/bijles/">Bijles (NL)</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Moment-Generating Functions for some discrete s.v.'s</h1> <p class="post-meta">August 20, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/statistics"> <i class="fas fa-tag fa-sm"></i> statistics,</a>   <a href="/blog/category/probability"> <i class="fas fa-tag fa-sm"></i> probability</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>We will consider some basic discrete probability models, and derive their expectance, variance and moment generating functions. My goal is to derive these in a calculus-y fashion. I hope you can fill in the analytic gaps.</p> <hr> <p>A discrete random variable is a real random variable \(X : \Omega \rightarrow \mathbb R\), but with a <em>support</em> (we will denote the support of \(X\) by \(\mathcal X\)) \(\mathcal X \subset \mathbb Z\), that is: \(\mathbb P [X \in A] = \mathbb P [X \in A \cap \mathbb Z]\).</p> <p>We will not use the probability measure directly but always assume that random variables can be characterized by their CDF \(F_\theta: x \mapsto \mathbb P [X \leq x]\) and the associated PDF (continuous) or PMF (discrete). Moreover, we assume that the random variable is real and either discrete or with an almost everywhere continuously differentiable CDF, we will call that a continuous CDF.</p> <p>The moment generating function (MGF) of a r.v. \(X\) is defined as:</p> \[M_X(t) = \mathbb E [e^{tX}]\] <p>If the implicit integral (or sum) converges.</p> <p>If \(X_1, X_2, ...\) is a sequence of independent r.v.’s then for any \(N \in \mathbb N\) we have:</p> \[M_{X_1 + ... + X_N} (t) = \mathbb E [e^{t(X_1 + ... + X_N)}] = \mathbb E [e^{tX_1}\cdot ... \cdot e^{tX_N}]\] <p>By independence of the \(X_i\), \(e^{tX_i}\) are independent, so we can pull the product through the \(\mathbb E\) and get:</p> \[M_{X_1 + ... + X_N} (t) = \mathbb E [e^{tX_1}]\cdot ... \cdot \mathbb E [e^{tX_N}] = M_{X_1}(t) \cdot ... \cdot M_{X_N}(t)\] <p>Which will be useful when we consider convolutions later.</p> <p>A model is simply a collection of CDF’s \(\{F_\theta \mid \theta \in \Theta \}\) where all \(F_\theta\) are either continuous or discrete CDF’s, for real-valued s.v.’s \(X\).</p> <p>We will write \(f_\theta\) for the PDF or PMF corresponding with \(F_\theta\).</p> <p>I will now skip the probability theoretical bureaucracy because that belongs in a textbook rather than in a blog.</p> <hr> <h2 id="bernoulli-or-alternative-distribution">Bernoulli or Alternative distribution</h2> <p>A probability model with a one-dimensional parameter $$p \in [0,1]</p> <p>\(X \sim \text{alt}(p)\) iff.</p> <ol> <li> \[\mathbb X = \{0,1\}\] </li> <li> \[f_p(x) = p \cdot 1_{\{x = 1\}} + (1-p) \cdot 1_{\{x = 0\}}\] </li> </ol> <p>It clearly has:</p> \[\mathbb E [X] = 1 \cdot p + 0 \cdot (1-p) = p\] \[\text {var} [X] = 1^2 \cdot p + 0^2 \cdot (1-p) - p^2 = p(1-p)\] \[M_X(t) = pe^{t\cdot 0} + (1-p) e^{t\cdot 1} = p + (1-p)e^t\] <p>The formula still holds in the edge cases \(\in \{0,1\}\).</p> <hr> <h2 id="binomial-distribution">Binomial distribution</h2> <p>Given a two-dimensional parameter $$(n,p) \in \mathbb N \times [0,1]</p> <p>\(X \sim \text{bin}(n,p)\) iff.</p> <ol> <li> \[\mathcal X = \{0,1, ... , n\}\] </li> <li> \[f_{n,p}(x) = \binom{n}{x}p^xq^{n-x}\] </li> </ol> <p>Where we define \(q = 1-p\) (it will later become clear that this simplifies calculations).</p> <p>An equivalent definition is that \(X\) follows the distribution of \(Y_1 + ... + Y_n\), where \(Y_i \sim \text{alt}(p)\) i.i.d. You can prove this rigorously by induction on N:</p> <blockquote> <p>Induction Basis: Clearly true for \(n = 1\) Induction Step:</p> <ul> <li>Suppose \(X_{n-1}\) has the same PMF as \(Y_1 + ... + Y_{n-1}\).</li> <li>Consider \(X_n = X_{n-1} + Y_n\)</li> <li>We can calculate \(\mathbb P [X_n = x]\) by partitioning over two events: \(Y_n = 0\) and \(Y_n = 1\):</li> <li>Then \(\mathbb P [X = x] = \mathbb P [X_n = x, Y_n = 0] + \mathbb P [X_n = x, Y_n = 1] = \mathbb P [X_{n-1} = x, Y_n = 0] + \mathbb P [X_{n-1} = x-1, Y_n = 1]\)</li> <li>Since \(X_{n-1}\) is only a function of \(Y_1, ... , Y_{n-1}\), the events for \(X_{n-1}\) and \(Y_n\) are independent:</li> <li>This gives \(\mathbb P [X = x] = \mathbb P [X_{n-1} = x]\mathbb P[Y_n = 0] + \mathbb P [X_{n-1} = x-1]\mathbb P [Y_n = 1]\)</li> <li>Use the induction hypothesis to substitute \(f_{n-1,p}(x)\) and \(f_{n-1,p}(x - 1)\) into the above equality. Also fill in \(\mathbb P [Y_n = 1] = p\), \(P[Y_n = 0] = q\)</li> <li>Actually, now you can finish the proof by only some algebra! Hint: <a href="https://en.wikipedia.org/wiki/Pascal%27s_rule" rel="external nofollow noopener" target="_blank">Pascal’s Identity</a>.</li> </ul> </blockquote> <p>Because of this, we now have some very easy derivations for the mean, variance and MGF:</p> \[\mathbb E X = \mathbb E [ \sum_{i=1}^n Y_i ] = \sum_{i=1}^n \mathbb E Y_i = np\] \[\text{var}[X] = \sum_{i = 1} ^n \text{var}[Y_i] = npq\] \[M_X(t) = M_{Y_1}(t) \cdot ... \cdot M_{Y_n}(t) = (p + 1 e^t)^n\] <hr> <h2 id="geometric-distribution">Geometric Distribution</h2> <p>We now have to be cautious and restrict \(p\) to the open interval: \(p \in (0,1)\).</p> <p>\(X \sim \text{geom}(p)\) iff.</p> <ol> <li> \[\mathcal X = \mathbb N_{\geq 0}\] </li> <li> \[f(x) = pq^{x-1}\] </li> </ol> <p>We can see that if \(Y_1, Y_2, ...\) is an infinite sequence of i.i.d. \(\text{alt}(p)\)-distributed r.v.’s, then \(T_1 = \min \{ k \in N_{\geq 0} \mid Y_k = 1\}\) is geometrically distributed with parameter \(p\), according to the above definition. This is because \(\mathbb P [ T_1 = x ] = \mathbb P [ Y_1 = 0, ... , Y_{k-1} = 0, Y_k = 1] = \mathbb P [ Y_1 = 0] ... \mathbb P [Y_{k-1} = 0] \mathbb P [Y_k = 1]\) by independence of the \(Y_i\)</p> <p>Where all the above derivations of expectations, variances and MGFs were done using finite sums, we now have a random variable with infinite support, and hence we are going to to do derivations using infinite series, in particular the geometric series:</p> \[\mathbb E X = \sum_{x = 1} ^ \infty xpq^{x-1}\] <p>Since \(\sum_{x = 1} ^ \infty y^{x}\) converges (absolutely) to \((1-y)^{-1}\) for all \(\vert y \vert &lt; 1\), \(\sum_{x = 1} ^ \infty py^{x}\) converges to \(p/(1-y)\), and we can even say that the series is term-wise differentiable w.r.t. \(y\) and thus has derivative \(\sum_{x = 1} ^ \infty xpy^{x-1}\), which for \(y = q\) is exactly our \(\mathbb E [X]\).</p> <p>So, we conclude \(\mathbb E X = \frac{d}{dy} \frac p {1-y} \vert_{y = q} = \frac p {(1-y)^2} \vert_{y=q} = \frac p {p^2} = p^{-1}\)</p> <p>We can do the variance in a similar way, but I will derive it via the MGF. Because if the MGF converges, it <em>generates the moments</em> of \(X\) because its taylor expansion in its variable \(t\) looks as follows::</p> \[M_X(t) = \sum_{x=0}^\infty \mathbb E [X^n] \frac {t^n} {n!}\] <p>In our case, the MGF is very simple:</p> \[M_X(t) = \sum_{x = 1} ^ \infty e^{tx} pq^{x-1} = \sum_{x = 1} ^ \infty p e^{[t+\ln(1-p)]x}\] <p>Which converges iff. \(e^{[t+\ln(1-p)]} &lt; 1\), which is the case iff. \(t &lt; -\ln(1-p)\)</p> <p>The second moment can thus be found by differentiating the MGF twice and evaluating in \(0\):</p> \[M_X '' (0) = \frac {d} {dt} \frac {pqe^t}{1 - qe^t} \vert_{t=0} = \frac {(1 - qe^t)^2pqe^t - pqe^t \cdot 2(q- qe^t) \cdot -qe^t} {(1 - qe^t)^4} \vert_{t = 0}\] \[= \frac{p^3q + 2p^2q^2} {p^4} = \frac {pq} {p^2} + \frac {2q^2} {p^2}\] <p>Then, we add in \((\mathbb E X) = (\frac q p)^2\). That will finally give us:</p> \[\text{var}[X] = \mathbb [X^2] - (\mathbb X)^2 = \frac {qp} {p^2} + \frac {q^2} {p^2} = \frac{p - p^2 + p^2 - 2p + 1} {p^2} = \frac q {p^2}\] <hr> <h2 id="inverse-binomial-or-polya-distribution">Inverse Binomial, or Polya distribution</h2> <p>We now want to model a sequence of experiments \(Y_1, Y_2, ...\) and the distribution of the number \(T_n\) at which the \(n\)th success occurs, in other words \(T_n : = \min \{ k \in \mathbb N \mid \sum_{i = 1} ^k Y_i = n\}\) Note that the set might not have a minimum in the event that all \(Y_i\) are \(0\) before \(n\) successes are reached. This was also the case for the geometric distribution. However, \(T_n\) is non-defective: this is because the event \(\cap_{i = l}^\infty \{Y_i = 0\}\) for any \(l\) has measure 0, for we have continuity of our probability measure:</p> \[\mathbb P [\lim_{N\rightarrow \infty} A_N] = \lim_{N \rightarrow \infty} \mathbb P [A_N]\] <p>and this together with:</p> <p>\(\mathbb P[\cap_{i = l}^N \{Y_i = 0\}] = q^N\),</p> <p>implies that indeed \(\mathbb P [\cap_{i = l}^\infty \{Y_i = 0\}] = \lim_{N\rightarrow \infty} q^N = 0\).</p> <p>Lovely little derivation. And we can conclude that the support of \(T_n\) is indeed \(\mathbb N \geq n\), and we don’t need to include the value “\(\infty\)” in \(\mathcal X\) (which is done in the case of a defective r.v., then we define the event \(\{T_n = \infty\}\) to be the event “\(\{ \ \{ k \in \mathbb N \mid \sum_{i = 1} ^k Y_i = n\} = \emptyset \ \}\)”, which may not have measure 0).</p> <p>To derive the PMF, we simply note that if we set \(Z_i = \sum_{j = 1} ^i Y_j\), then \(Z_i \sim \text{bin}(i,p)\), and \(Z_i\) and \(Y_{i+1}\) are independent, so that:</p> \[f(x) = \mathbb P [Y_x = 1, Z_{x-1} = n-1] = \mathbb P [Y_x = 1]\mathbb P[ Z_{x-1} = n-1] = p \cdot \binom {x-1} {n -1} p^{x-1} q^{x-1 - (n-1)} = \binom {x-1} {n -1} p^xq^{x-n}\] <p>To derive the MGF, I will try another good trick: We can argue that the number of experiments needed for a next success, \(\{T_k - T_{k-1}\}_{1\leq k \leq n}\), is an independent set of i.i.d. \(\text{geom}(p)\) r.v.’s. In that case, if we put \(S_k := T_k - T{k-1}\) for \(1 \leq k \leq n\) (Yes, I see you, but we can simply define \(T_0 \equiv 0\)), we get:</p> \[M_{T_n} = M_{S_1}(t)\cdot ... \cdot M_{S_n}(t) = (\frac p {1 - qe^t})^n\] <p>And from this MGF we can again derive the first moment:</p> \[\mathbb E T = M'_{T_n}(0) = \frac d {dt} p^n (1 - qe^t)^{-n} = (-n)(-q)p^n(1-q)^{-n-1} = nqp^{-1} = n \mathbb E S_i\] <p>Alternatively, we can also argue that the expectation of a sum of independent variables is the sum of the expectations: \(\)\mathbb E T = n \mathbb E S_i = nqp^{-1}$$. The same holds for the variance:</p> \[\text{var}[T_n] = \sum_{i=1}^n \text{var}[S_i] = n p^{-2}q\] <hr> <h2 id="poisson-distribution">Poisson distribution</h2> <p>What if we would consider a sequence of binomial distributions \((\text{bin}(n, p_n))_{n=0}^\infty\) such that \(n\cdot p_n \rightarrow \lambda\) as \(n\rightarrow \infty\)? You already know the answer: Poisson! Let’s skip Stirling’s approximation and the entire derivation that comes after, and state the PMF:</p> <p>We have one parameter which is usually \(\lambda\) and we require \(\lambda &gt;0\).</p> <ol> <li> \[\mathcal X = \mathbb N_{\geq0}\] </li> <li> \[f(x) = e^{-\lambda}\frac {\lambda^x}{x!}\] </li> </ol> <p>From this we can derive the MGF:</p> \[M_X(t) = \sum_{x=0}^\infty e^{tx}e^{-\lambda}\frac{\lambda^x}{x!} = e^{-\lambda}\sum_{x=0}^\infty \frac {e^{[\log(\lambda)+t]x}}{x!} = e^{-\lambda}e^{e^{[\log(\lambda)+t]}} = e^{\lambda(e^t-1)}\] <p>The first moment is \(\mathbb E X = M_X ' (0) = e^{\lambda(e^t-1)}\cdot \lambda e^t \vert_{t=0} = e^0\cdot\lambda\cdot e^0 = \lambda\)</p> <p>The second moment is \(\mathbb E X = M_X '' (0) = [\frac d {dt} e^{\lambda(e^t-1)}\cdot \lambda e^t ]_{t=0} = \lambda^2 + \lambda\), giving \(\text{var}[X] = \lambda^2 + \lambda - \lambda^2 = \lambda\) as well.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/simulating-geometric-distribution/">Simulating the Geometric distribution in constant time</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/logic-model-theory/">Logic, Models, Proofs</a> </li> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Matthijs Muis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 30, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
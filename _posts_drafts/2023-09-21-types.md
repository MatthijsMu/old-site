

In modern programming languages such as c++ or rust, every object comes with a type.

## Definition of type

An object is meant to be a unified unit of program data. A type is something that is hard to give a concrete definition. Like sets, it is easier to describe them axiomatically. This is the
apporach of
type theory, an analogon of set theory (and, in fact, also a
branch of modern logic).

If you are familiar with axiomatic set theory, you know that we can work with sets by stating axioms about them, such as *extensionality* : $A = B$ iff. $\forall a : a\in A \leftrightarrow a \in B$.

Rather than giving a definition of a type, we can say a type is
a name that is associated with certain values that an object of 
that type can take, and certain operations that can be done on
objects of that type. This idea is described by the first few axioms of type theory. Type theory then goes on with axioms on how new types can be made from old. For example, for $A$ and $B$ types, we may have a type $A\rightarrow B$ (which we can interpret asd the type of all functions $A\rightarrow B$). And if $f$ is an instance of type $A\rightarrow B$, and $a$ an instance of $A$, then $fa$ is an instance of $B$. In fact, we can define the type $A\rightarrow B$ in this way.

## Functional languages

The details are more intricate than this, but I hope this gives you the main idea. Languages implementing the ideas of type theory (at least partially) are mostly *functional programming* languages such as Haskell and ML. Compilers for haskell come with a type inference system that will warn the programmer and not compile the code to (untyped machine code) if there are constraints from the axioms of type theory violated. A reason to program in functional languages is that the compiler will check you for these kinds of errors. 

A consequence of the *functional* approach of these languages (that is, the main kind of operation enforced by the *style* of this languages)  is that the programmer can be certain of the absence of *side-effects* in part of the program code that do not explicitly state that they are going to do these side effects (i.e. via the use of monads).

## Types should not be taken for granted

We often forget that types are a very special and helpful language construct. But remember, types do not exist (in the same sense as they exist in statically typed languages) on the hardware level to begin with. 

You could say that the unit of execution in the CPU, a single instruction operating on data, gives the actual semantics  to its operands, and hence there are types. 

Take for example the instruction `add r1 r2 r1`, which adds the contents register `r1` and `r2` and stores this in register `r1`. It is in the *interpretation of the programmer* that the bitstrings at `r1` and `r2` represent certain integer values, say in binary, big-endian encoding, and that the way these are
passed through the ALU circuit results, semantically, in an addition (that is, the bitstring that comes out of the ALU encodes exactly the integer resulting from adding `r1` and `r2` interpreted as integers).

So in a sense, even though we never add bits to the data to encode
the type, we can interpret the CPU hardware as having certain
types due to the fixed set of operations (around 100 instructions) it can do on the data. But this is not what we call a type! Types are not semantics arising from operations, they are inherently part of the operands and should remain constant unless otherwise specified by the programmer, during the *whole lifetime* of the object.

The difference that makes machine code an untyped language is
that the type of data is never enforced, that is, the same data
at one location in memory can be operated upon by an instruction 
interpreted to be valid for integers but also by an operation for unsigned integers. The type can be seen as only present during 
the operation, but it is not enforced during the lifetime of the
object. 

Moreover, machine code does not have an option of declaring values to be of a certain type (we would need to add bits that communicate the type on the hardware level, and this is not done, because most hardware implements instruction set architectures that don't support types to begin with).

## Why types are useful

It may seem like quite a clerical chore to declare types before defining and instantiating objects in your code.
Maybe during your first programming experience, you could not
even imagine that types are useful and that they only cause 
the programmer another source of bureaucracy to deal with.

This is at least what I thought when I started programming in C++. It might even be one of the reasons that I quickly switched to Python (apart from the array out-of-bounds errors). But I think that I have changed my view on this. I at least feel very productive in languages like Haskell, where I am able to write very complicated algorithms in just a few lines of code, while also being ascertained by the compiler that I at least made no mistake in the types that I used (which is, looking at the number of errors I typically get, a large part of the bugs I would otherwise have caused). 

Python does not require types, you are free to reinterpret data of one type as another and it might actually support you in this by recasting appropriately. But in the light of avoiding errors due to Python being too flexible with its types, I can recommend using type hints, which is 
essentially a form of static typing that can nowadays be added to
Python code.

<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://matthijsmu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://matthijsmu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-20T20:07:52+00:00</updated><id>https://matthijsmu.github.io/feed.xml</id><title type="html">blank</title><subtitle>Matthijs Muis, Undergraduate student Mathematics, Computer Science at Radboud University Nijmegen. Portfolio + Teaching + Blog + Personal Stuff. </subtitle><entry><title type="html">Set Theory 2</title><link href="https://matthijsmu.github.io/blog/2023/set-theory-2/" rel="alternate" type="text/html" title="Set Theory 2"/><published>2023-08-15T15:09:01+00:00</published><updated>2023-08-15T15:09:01+00:00</updated><id>https://matthijsmu.github.io/blog/2023/set-theory-2</id><content type="html" xml:base="https://matthijsmu.github.io/blog/2023/set-theory-2/"><![CDATA[<p>In the last post, we discussed the first 6 axioms of set theory. <em>Extensionality</em> defined when two sets are equal: in particular, it expressed \(a = b\) in a sentence involving only the elementhood relation symbol \(\in\). The <em>Emtpy set</em> axiom stated the existence of a set without members. <em>Pairing</em>, <em>Union</em>, <em>Power set</em> and <em>Separation</em> provided ways of constructing new sets from old. We were able to prove some basic theorems and define some basic new sets, such as the <em>cartesian product</em> \(A\times B\) of two sets \(A, B\).</p> <p>We will now define relations and study relations with certain properties. These are <em>order</em> relations, <em>equivalence</em> relations and <em>functions</em>.</p> <hr/> <h6 id="relations">Relations</h6> <blockquote> <p><strong>Definition</strong> A relation on \(A\times B\) is a subset \(R \subset A\times B\).</p> </blockquote> <blockquote> <p><strong>Definition</strong> A relation on \(A\times A\) is also called a relation on \(A\).</p> </blockquote> <p>By convention, we denote \(a R b\) if \((a,b) \in R\). Trivial example relations on \(A\) are:</p> <blockquote> \[I_A \equiv \{(a,b)\in A : a = b \}\] </blockquote> <blockquote> \[\subset_A \equiv \{(a,b)\in P(A)\times P(A) : a \subset b \}\] </blockquote> <hr/> <h6 id="pre-orders-partial-orders-total-orders">Pre-orders, partial orders, total orders</h6> <p>A relation \(R\) on \(A\) can satisfy various interesting axioms:</p> <blockquote> <p><strong>Transitivity</strong> <em>For all</em> \(a\in A, b \in A, c \in A\) (\(aRb\) <em>and</em> \(bRc\)) <em>then</em> \(aRc\)</p> </blockquote> <blockquote> <p><strong>Reflexivity</strong> <em>For all</em> \(a \in A\) \(aRa\)</p> </blockquote> <p>A pre-order is any relation that satisfies Transitivity and Reflexivity. A <em>partial</em> order is a pre-order that also satisfies <em>Antisymmetry</em>:</p> <blockquote> <p><strong>Antisymmetry</strong> <em>For all</em> \(a\in A, b\in B\) (\(aRb\) <em>and</em> \(bRa\)) <em>then</em> \(a=b\)</p> </blockquote> <p>The wording “Antisymmetry” is very well-chosen here. Because we see exactly that there is no “symmetry” in \(R\) between any two different elements a and b, that is, we cannot exchange \(a\) with \(b\) and get another element \((b,a)\in R\).</p> <p>An example of a partial order is that of the natural order \(\leq\) on the real numbers. Note that this order is however also <em>total</em>.</p> <blockquote> <p><strong>Totality</strong> <em>For all</em> \(a\in A, b\in B\) (\(aRb\) <em>or</em> \(bRa\))</p> </blockquote> <p>A partial order is exactly <em>partial</em> in the sense that Totality need not hold. An example is the relation \(\subset_A\) for any \(A\) that is not \(\emptyset\) or a singleton set. In that case, we can find an \(a\in A\) and a \(b\in A\) such that \(a\not = b\), and we clearly see that \(A \backslash \{a\} \not \subset_A A \backslash \{b\}\) but also \(A \backslash \{b\} \not \subset_A A \backslash \{a\}\), so clearly the sets are incomparable.</p> <hr/> <h5 id="equivalences-orbits-quotient-sets-partitions">Equivalences, orbits, quotient sets, partitions.</h5> <p>The pre-order can also be built out into a completely different direction, namely by adding an axiom that says exactly the opposite of <em>Antisymmetry</em>: we don’t <em>exclude</em> the possibility that both \(aRb\) and \(bRa\) hold simultaneously, but <em>require</em> it. Not surprisingly, we call this axiom <em>symmetry</em>.</p> <blockquote> <p><strong>Symmetry (1)</strong> <em>For all</em> \(a \in A, b\in B\) (\(aRb\) <em>then</em> \(bRa\))</p> </blockquote> <p>Because we can exchange \(a\) and \(b\), the <em>then</em> can equally well be replaced by an <em>iff.</em>.</p> <blockquote> <p><strong>Symmetry (2)</strong> <em>For all</em> \(a \in A, b\in B\) (\(aRb\) <em>iff.</em> \(bRa\))</p> </blockquote> <p>What version of the axiom we use depends on what we find convenient. A <em>then</em> statement is often a bit shorter to prove, while assuming an <em>iff.</em> statement usually makes the proof of some other property a bit shorter.</p> <p>Assuming a set \(X\) with equivalence relation \(R\), we can define for any \(x \in X\) the <em>orbit</em> \([x]\).</p> <blockquote> <p><strong>Orbit</strong> Also called the <em>equivalence class</em> of the element \(x\). It is denoted and defined \([x] \equiv \{y\in X : xRy \}\)</p> </blockquote> <p>An interesting property of orbits is that \(y\in [x]\), iff. (\(y\in X\) and \(xRy\)), which is by symmetry iff there is a set \([y]\), \(x\in X\) and \(yRx\)), so iff \(x \in [y]\). In particular, we have the following lemma:</p> <blockquote> <p><strong>Lemma</strong> \([x] = [y]\) <em>iff.</em> \(xRy\)</p> </blockquote> <blockquote> <p><strong>Proof</strong> We have to prove both directions:</p> <ul> <li>Suppose \([x] = [y]\). Then since \(xRx\) by reflexivity, \(x\in [x]\), so \(x\in [y]\) by Extensionality. It follows by definition of \([y]\) (and some applications of symmetry) that \(xRy\)</li> <li>Suppose \(xRy\). By Symmetry, we also have \(yRx\). hence if \(z\in [x]\), then \(zRx\), so by Transitivity \(zRy\), so by definition \(z\in [y]\). To show that every \(z\in [y]\) is also in \([x]\), we do exactly the same. Hence by Extensionality \([x]=[y]\).</li> </ul> </blockquote> <p>We can use this to show that orbits either coincide or are disjoint:</p> <blockquote> <p><strong>Lemma</strong> Either one of the following two alternatives holds:</p> <ul> <li> \[[x]=[y]\] </li> <li> \[[x]\cap [y] = \emptyset\] </li> </ul> </blockquote> <blockquote> <p><strong>Proof</strong> We either have \(xRy\) or \(x\not R y\).</p> <ul> <li>If \(xRy\) we conclude by the preceding lemma that \([x]=[y]\), in which case [x]\cap [y] are clearly not disjoint because they are <em>non-empty</em> (this is important!) and equal.</li> <li>If \(x\not R y\), we cannot have a \(z \in [x]\cap [y]\), since if such a \(z\) were to exist, we would have \(xRz\) and \(zRy\), so transitivity would give us that \(xRy\), a contradiction.</li> </ul> </blockquote> <p>From the orbits, we define the <em>quotient set</em> \(X/R\).</p> <blockquote> <p><strong>Quotient set</strong> This set is denoted \(X/R \equiv \{B \in P(X) :\) <em>Exists</em> \(x \in X \ [x]=B \}\)</p> </blockquote> <p>By the preceding lemma and some additional bookkeeping, we can see that quotient sets are all <em>partitions</em>.</p> <blockquote> <p><strong>Partition</strong> A <em>partition</em> of a set \(X\) is a set \(\pi \subset P(X)\) satisfying:</p> <ul> <li><em>Disjointness of Parts</em> \(B, C \in \pi\) with \(B \not = C\) are disjoint</li> <li><em>Coverage</em> \(\cup \pi = X\)</li> <li><em>Non-emptiness of Parts</em> \(B \in \pi\) <em>then</em> \(B \not = \emptyset\), or equivalently \(\emptyset \not \in \pi\)</li> </ul> </blockquote> <p>Let’s prove this:</p> <blockquote> <p><strong>Theorem</strong> If \(R\) is an equivalence relation on \(X\) then \(X/R\) is a partition of \(X\).</p> </blockquote> <blockquote> <p><strong>Proof</strong></p> <ul> <li>Let \(B, C\) be in \(X/R\). These are, by definition of \(X/R\), both equivalence classes of one or another element of \(X\), say \(B = [x]\) and \(C = [y]\) with \(x\in X,y\in X\). We assume \(C \not = B\), so that \([x]\not = [y]\) and hence they must be disjoint.</li> <li>let \(x\in X\), then \(x\in [x]\) since \(xRx\) (Reflexivity), and hence \(x\) is an element of \([x]\) which is an element of \(X/R\). So \(x \in \cup(X/R)\) for all \(x\in X\), but since \(X/R \subset P(X)\) we also have \(\cup (X/R) \subset X\), hence \cup (X/R) = X$$.</li> <li>Any \(B \in X/R\) is an equivalence class of some \(x\in X\), hence \(x\in B\), so \(\emptyset \not \in \pi\).</li> </ul> </blockquote> <p>Conversely, every partition \(\pi\) of \(X\) leads to an equivalence relation on \(X\), by setting \(xRy\) for \(x\in X, y\in X\) only if there exists a \(B\in \pi\) with \(x\in B\) and \(y\in B\).</p> <blockquote> <p><strong>Theorem</strong> This makes \(R\) and equivalence relation on \(X\).</p> </blockquote> <blockquote> <p><strong>Proof</strong></p> <ul> <li><em>Symmetry</em>: suppose \(xRy\). Then there is a \(B\in pi\) such that \(x\) and \(y\) are element of that set. So we might as well say that “there is a \(B\in pi\) such that \(y\) and \(x\) are element of that set”, which defines \(yRx\).</li> <li><em>Transitivity</em>: suppose \(xRy\) and \(yRz\). Then there is a \(B\in \pi\) with \(x\in B\) and \(y\in B\), and there is also a \(C \in \pi\) with \(y\in C\) and \(z\in C\). But then \(B\cap C \not = \emptyset\) since \(z\) is in this intersection, and so \(B = C\) by Disjointness of Parts. This also means that there is a \(C\in \pi\) such that \(x\in C\) and \(z \in C\), which leads to \(xRz\).</li> <li><em>Reflexivity</em>: This is the simplest part. By Coverage, every \(x\in X\) is contained in \(\cup \pi\), so take \(x\in X\), then there is some \(B\in \pi\) such that \(x\in B\), and hence both \(x\) and \(x\) are in some \(B\in \pi\). This defines \(xRx\).</li> </ul> </blockquote> <p>Using these constructions, we can even show that there is a bijective correspondence between the set of all equivalence relations on \(X\) and the set of all partitions of \(X\). But we have not discussed bijections, let alone functions yet.</p>]]></content><author><name></name></author><category term="set-theory"/><summary type="html"><![CDATA[Discusses relations, in particular orders, equivalences.]]></summary></entry><entry><title type="html">Set Theory 3 (Unfinished)</title><link href="https://matthijsmu.github.io/blog/2023/set-theory-3-unfinished/" rel="alternate" type="text/html" title="Set Theory 3 (Unfinished)"/><published>2023-08-15T15:09:01+00:00</published><updated>2023-08-15T15:09:01+00:00</updated><id>https://matthijsmu.github.io/blog/2023/set-theory-3-unfinished</id><content type="html" xml:base="https://matthijsmu.github.io/blog/2023/set-theory-3-unfinished/"><![CDATA[<h2 id="functions">Functions</h2> <p>A function can now be defined as follows:</p> <blockquote> <p><strong>Function</strong> A function \(f : A\rightarrow B\) is a relation \(f\subset A\times B\) such that <em>For all</em> \(x \in A\) <em>!Exists</em> \(y \in Y\) \(xfy\)</p> </blockquote> <p>We just introduced a new shorthand notation, which I shall define:</p> <blockquote> <ul> <li>(\(!Exists y \in Y\) \(\varphi\)) <em>iff.</em> (\(!Exists y\) (\(y \in Y\) <em>and</em> \(\varphi\)))</li> <li>(\(!Exists y\) \(\psi\)) <em>iff.</em> ((\(Exists y\) \(\psi\)) <em>and</em> (<em>For all</em> z (\(\varphi[z/y]\) <em>iff.</em> \(z=y\)))</li> </ul> </blockquote> <p>That means, the \(y\) satisfying \(xfy\) is unique for every \(x\).</p> <h2 id="axiom-of-replacement">Axiom of replacement</h2> <p>Suppose we have a well-formed formula \(\varphi(x,y)\) that only has free variables \(x\) and \(y\), such that for all sets \(a\), there is at most one set \(b\) for which \(\varphi(a,b)\) holds. We would like to have a set \(B\) of all such \(b\). We immediately think of using the axiom of separation for this, but what is the set to separate the \(b\)’s from? Since there is no universal set:</p> <blockquote> <p><strong>Theorem</strong> There is no set \(U\) such that if \(a\) is a set then \(a\in U\). <strong>Proof</strong> Suppose we had such a \(U\). Then let us use Separation to define the set \(R = \{x\in U : x\not \in x\}\). We get Russel’s paradox again by asking ourselves whether \(U\in U\) or not.</p> </blockquote> <p>We cannot use any of the preceding axioms to construct the desired \(B\). But we would like to have a function \(f:D_A\rightarrow B\), where the domain \(D_A = \{x\in A:\) <em>Exists</em> \(y\) \(\varphi(x,y)\}\) can be separated from any set \(A\).</p> <p>The replacement axiom assumes that we can do it. Like the separation axiom, it is a schema axiom over all wff.’s \(\varphi\).</p> <blockquote> <p><em>For all</em> \(A\) <em>Exists</em> \(B\) <em>For all</em> \(b\inB\) <em>Exists</em> \(x\in A\) \(\varphi(x,b)\)</p> </blockquote> <p>(Unfinished)</p> <p><em>In order to construct the real numbers from this point onwards, we would need to do the following:</em></p> <ul> <li><em>Define for any set \(a\), the successor to be \(a^+ := a\cup\{a\}\). A successor set is a set \(A\) with \(\emptyset\in A\) and for every \(a\in A\) also \(a^+\in A\)</em>. We don’t know yet whether there are successor sets.</li> <li><em>Add the axiom of Infinity to assure there is an infinite set containing \(\emptyset\), \(\emptyset\cup\{\emptyset\}\), etcetera, so a set \(I\) with \(\emptyset \in I\) and for \(a\in I\) we have \(a^+\in I\). This is not guaranteed by the other axioms, because in formal proofs we can only use a finite number of deductions: thus, we can only construct finite sets containing “up to \(n\)” successors of \(\emptyset\) from pairing, union and emptyset alone and need an axiom to postulate such an infinite set.</em></li> <li><em>Let \(\omega\) be the intersection of all successor sets: this set also is a successor set, since it has \(\emptyset\) and all successors of its elements. It is also minimal.</em></li> <li><em>Add the axiom of regularity, which states that if \(A\not = \emptyset\) then there is a \(a\in A\) s.t. \(a\cup A = \emptyset\). From this we conclude \(a\not \in a\) for any set \(a\), because if it were then \(a\cup\{a\} \not = \emptyset\), which is clearly in contradiction with the axiom.</em></li> <li><em>Show that \(\omega\) models the axioms of Peano Arithmetic (PA): in particular, we need the axiom of regularity to show \(m^+ \not = n^+\) if \(m\not=n\).</em></li> <li><em>Construct the integers \(\mathbbZ\) from equivalence classes of pairs of natural numbers.</em></li> <li><em>Conclude that the \(\mathbbZ\) models the commutative ring axioms, and form an infinite set. Also conclude that that \(\mathbbZ\) forms a domain, that is, it has no zero divisors</em></li> <li><em>Use the concept of the localization of rings \(R\) at a multiplicatively subset \(S\subset R\) to construct from \(S = \mathbbZ \backslash \{0\}\) the rational numbers \(\mathbbQ = S^{-1}\mathbbZ\)</em></li> <li><em>Define Dedekind-cuts on the rationals.</em></li> <li><em>Model the real numbers as Dedekind cuts.</em></li> </ul> <p>In retrospect, this is far too tedious for a blog post. Read Garling or <em>Sets, Models and Proofs</em> instead. They are excellent sources on this.</p>]]></content><author><name></name></author><category term="set-theory"/><summary type="html"><![CDATA[Discusses functions.]]></summary></entry><entry><title type="html">Set Theory 1</title><link href="https://matthijsmu.github.io/blog/2023/set-theory-1/" rel="alternate" type="text/html" title="Set Theory 1"/><published>2023-08-04T15:09:01+00:00</published><updated>2023-08-04T15:09:01+00:00</updated><id>https://matthijsmu.github.io/blog/2023/set-theory-1</id><content type="html" xml:base="https://matthijsmu.github.io/blog/2023/set-theory-1/"><![CDATA[<h2 id="the-notion-of-a-set-is-something-very-intuitive-that-we-use-everyday-yet-a-rigorous-definition-of-a-set-is-when-you-first-try-this-by-yourself-quite-difficult-to-give-there-seems-to-be-nothing-more-elementary-than-the-set-itself-so-it-is-hard-to-find-something-below-the-set-to-describe-a-set-with">The notion of a set is something very intuitive that we use everyday. Yet a rigorous definition of a set is, when you first try this by yourself, quite difficult to give. There seems to be nothing more elementary than the set itself, so it is hard to find something “below the set” to describe a set with.</h2> <h4 id="preliminaries-some-naive-logic">Preliminaries: some naive logic</h4> <p>Our general approach is to use axioms, which are logical sentences according to which any set behaves. Logical sentences are defined from <em>well-formed logical formulas</em> or <em>wff</em>’s. A <em>wff</em> is nothing more than a string of logical symbols, which in our case will be <em>Exists</em>, <em>For all</em>, <em>not</em>, <em>then</em>, <em>iff.</em>, <em>and</em>, <em>or</em>, together with variables we will often denote with lowercase letters, such as \(x\), or constants (which are known objects with a name) or abstract sentences often denoted by an uppercase letter such as \(P\) or \(Q\). We will also introduce the symbol \(=\). Finally, there are <em>relation</em> and <em>function</em> symbols that are specific to the theory, or define symbols from others using sentences. These are, among others \(\in\),\(\subset\),\(\cap\), \(\cup\).</p> <p>We will think of the logical operations very loosely, as having the logical meaning as in everyday language. This mixture of symbols and their meaning is very different from the treatment in rigorous formal logic, which strictly separates the symbols (syntax) from their interpretation (semantics). Strings of these symbols The logical operations in these sentences we shall not formalize here, we only require knowledge of logic on the level used in everyday reasoning.</p> <p>Precisely, a wff \(P\) is one of the below:</p> <p>A “composite” wff:</p> <ul> <li>(<em>not</em> (\(Q\))), where \(Q\) is a wff.</li> <li>(<em>Exists</em> \(x\) (\(Q\))), where where \(Q\) is a wff and \(x\) a variable.</li> <li>(<em>For all</em> \(x\) (\(Q\))), where where \(Q\) is a wff and \(x\) a variable.</li> <li>((\(Q\)) <em>then</em> (\(R\))), where \(Q\), \(R\) are wff’s.</li> <li>((\(Q\)) <em>iff.</em> (\(R\))), where \(Q\), \(R\) are wff’s.</li> <li>((\(Q\)) <em>and</em> (\(R\))), where \(Q\), \(R\) are wff’s.</li> <li>((\(Q\)) <em>or</em> (\(R\))), where \(Q\), \(R\) are wff’s.</li> </ul> <p>Or, an “atomic” wff:</p> <ul> <li>\(t = s\), where \(t\) and \(s\) are <em>terms</em>.</li> <li>\(R(t_1, t_2, ..., t_n)\) where \(R\) is an \(n\)-ary relation symbol and \(t_1, ... , t_n\) are terms.</li> </ul> <p>A <em>term</em> is then simply:</p> <ul> <li>\(f(t_1, t_2, ..., t_n)\), where \(f\) is an \(n\)-ary function symbol and \(t_1, ..., t_n\) are terms.</li> <li>\(c\), where \(c\) is a constant.</li> <li>\(x\), where \(x\) is a variable.</li> </ul> <p>Sentences are simply wff’s in which for each variable \(x\) occurring, it occurs in a sentence \(P\) that is nested in a quantifier, that is in a subsentence of the form (<em>Exists</em> \(x\) (\(P\))) or</p> <p>The logical equivalences (or even the definition of <em>logical equivalence</em>) between various nestings of operations is assumed to be familiar to you. Also note that we interpret <em>or</em> as <em>inclusive</em>, that is, one of \(Q\), \(R\) or both need to hold in order to make a sentence (\(Q\) <em>or</em> \(R\)) <em>true</em>.</p> <p>One may ask why I am not using the classical logical symbols \(\exists\), \(\forall\), \(\lnot\), \(\land\), \(\lor\), \(\rightarrow\), etcetera, for writing logical sentences. The reason is that I personally think that if we are going to work with them on a metalevel anyway, and on the metalevel we inevitably will mix semantics and syntax, then it is pretentious to use the logical symbols used to study classical logic. The statements that we make are namely no more precise than our understanding of our natural language. On the other hand, using full-length sentences in a natural language is as readable as it is cumbersome to write.</p> <p>In set theory, we only use one binary (2-ary) relation symbol to describe sets, namely \(\in\), and we write it <em>infix</em>. The other properties can be expressed using logical sentences and this relation. \(\in\) is the relation of <em>elementhood</em>. The idea is that one set, say, \(a\), can be an element of another, \(b\), which we denote by \(a \in b\).</p> <hr/> <h4 id="axiom-1-extensionality">Axiom 1: Extensionality</h4> <p>Two sets \(a\) and \(b\) are equal, denoted \(a = b\), precisely when every element of \(a\) is an element of \(b\) and every element of \(b\) is an element of \(a\). We can express this in first-order logic in the following way:</p> <blockquote> <p>(\(A = B\)) iff. (<em>For all</em> \(x\) ( \(x \in A\) iff. \(x \in B\)))</p> </blockquote> <p>This formula holds for all sets, so technically we would make this into an axiom by prepending “<em>For all</em> \(A\) <em>For all</em> \(B\)”</p> <blockquote> <p><em>For all</em> \(A\) <em>For all</em> \(B\) ((\(A = B\)) iff. (<em>For all</em> \(x\) ( \(x \in A\) iff. \(x \in B\))))</p> </blockquote> <p>I am already ignoring some bracketing for readability and laziness, and this will get worse over time! Again, I think that the exact structure of the logical sentence is clear to you, and sometimes readability is, at least for <em>human</em> readers, more important than well-formedness.</p> <p>The above sentence is the first axiom of set theory, named “Extensionality”, “axiom of extension” or “the extension axiom”. In a sense, it defines the “meaning” \(a = b\) for sets \(a\) and \(b\). We avoided having to explain “what” a set is by appealing to what it means for two sets to be equal. Again, “meaning” only exists as an interpretation, which is really the area of model theory, but we don’t care about that in this stage. First we develop a meta-level understanding of sets, and only after developing rigorous mathematical logic can we embed the axioms of set theory in a first-order logical language.</p> <p>We will now leave ourselves leave the philosophical interpretations of axioms and theories and state the other axioms of set theory.</p> <p>Having defined equality for the sets that we have, we might ask: what sets <em>do</em> we actually <em>have</em>? A first attempt what to define sets from wff’s involving only one free variable, say \(P(x)\) involving a variable \(x\) which is the only one which is free. This way of defining what sets exists is called the <em>Principle of comprehension</em>.</p> <p>It fails due to Russel’s paradox:</p> <blockquote> <p><strong>Russel’s paradox</strong> Let \(S\) be the set of all \(x\) such that \(x \in x\). Then not \(S \not \in S\), nor \(S \in S\).</p> </blockquote> <blockquote> <p><strong>Proof</strong>: Notice that we have applied the axiom of comprehension to the wff \(P(x) \equiv (x \in x)\). \(S \in S\), then \(S\) satisfies \(P\), hence \(S \not \in S\), leading to a contradiction. If \(S \not \in S\), then \(S\) does not satisfy \(P\), hence \(S \in S\), which is also a contradiction.</p> </blockquote> <p>The axiom of comprehension leads to contradictions so we do not want to use it to do set theory. This is because logically, anything follows from a contradiction, so we get a garbage theory with which we can prove anything we want and don’t want about sets. This is why we need 9 or 8 (or 7, depending on whether you like the axiom of choice) more axioms to define what sets can actually exist.</p> <hr/> <h4 id="axiom-2-empty-set">Axiom 2: Empty set</h4> <p>First, we postulate that there is a set which has no members, i.e.</p> <blockquote> <p><em>Exists</em> \(x\) <em>For all</em> \(y\) \(y \not \in x\)</p> </blockquote> <p>Say that \(B\) is a set that satisfies this sentence for \(x\). By extensionality, if another set \(C\) also has no elements, then \(B = C\) must hold by Extensionality. So the set \(B\) must be unique. Let’s give it a (famous) name: \(\emptyset\).</p> <p>Having given \(\emptyset\) as a basis from which to build new sets, we next consider axioms that state the existence of new sets based on existent sets. These define us rules to create new sets from existent sets.</p> <hr/> <h4 id="axiom-3-pairing">Axiom 3: Pairing</h4> <blockquote> <p><em>For all</em> \(x\) <em>For all</em> \(y\) <em>Exists</em> \(p\) (<em>For all</em> z (\(z \in p\)) <em>iff.</em> z (\(z = x\)) <em>or</em> (\(z = y\)))</p> </blockquote> <p>This basically states that if \(x\) and \(y\) are sets, then there must also be a set \(p\) which has exactly \(x\) and \(y\) as its elements. By Extensionality, such a \(p\) is a uniquely defined set. That is why the notation \(\{ x, y \}\) makes sense: the elements \(x\) and \(y\) uniquely define \(p\). Note that order doesn’t matter: \(\{ x, y \} = \{ y, x \}\), since they have the same elements. In general, we can define <em>finite</em> sets with “curly braces”-notation by just denoting their elements. Also note that if we apply the pairing axiom to \(x\) and \(x\), we arrive at the existence of \(\{ x, x \}\), but this is just \(\{x\}\) because a set is uniquely defined by which elements it contains, and \(y \in \{x, x \}\) <em>iff.</em> \(y = x\), which is “<em>iff.</em> \(y \in \{x\}\).</p> <p>The pairing axiom enables us to make a new set apart from \(\emptyset\) , namely \(\{ \emptyset \}\). This set is clearly different from \(\emptyset\), because it has an element while \(\emptyset\) does not.</p> <p>Pairing also enables us to define <em>ordered pairs</em>. For two sets \(a\), \(b\), we define the ordered pair \((a, b)\) to be \(\{\{a\},\{a,b\}\}\). The definition enforces an “order” on the elements of the pair, in the sense that:</p> <blockquote> <p><strong>Theorem</strong> \((a,b) = (c,d)\) <em>iff.</em> (\(a = c\) <em>and</em> \(b = d\)).</p> </blockquote> <blockquote> <p><strong>Proof</strong> We distinguish two cases:</p> <ul> <li>If \(a = b\), we have by definition \((a,b) = \{\{a\},\{a,a\}\} = \{\{a\},\{a\}\} = \{\{a\}\}\). If \(c\not = d\), then \((c,d)\) will contain two distinct elements, which leads to a contradiction by Extensionality. Hence \(c = d\) and we have \(\{\{c\}\} = (c,d) = (a,b) = \{\{a\}\}\). Then by Extensionality, \(\{a\} = \{c\}\). Applying Extensionality again, \(a = c\).</li> <li>If \(a \not = b\), we observe that \(\{a\}\) and \(\{a,b\}\) must be distinct so by applying Extensionality to \((a,b) = (c,d)\), we conclude that \(\{c\}\) and \(\{c,d\}\) must be distinct as well, and we either have: <ul> <li>\(\{a\} = \{c,d\}\) and \(\{a,b\} = \{c\}\), which is clearly not possible as \(c \not = d\) so \(\{c,d\}\) is not a singleton set while \(\{a\}\) is.</li> <li>Thus, the second case, \(\{a\} = \{c\}\) and \(\{a,b\} = \{c,d\}\), must hold. Then by applying Extensionality to the first equality, we conclude \(a = c\). Since \(b \not = a = c\), the second equality leads to \(b = d\) necessarily.</li> </ul> </li> </ul> </blockquote> <hr/> <h4 id="axiom-4-union">Axiom 4: Union</h4> <blockquote> <p><em>For all</em> \(x\) <em>Exists</em> \(y\) <em>For all</em> \(z\) (\(z \in y\) <em>iff.</em> (<em>Exists</em> \(s\) (\(s \in x\) <em>and</em> \(z \in s\))))</p> </blockquote> <p>The axiom says that if \(x\) is a set then there is a set \(y\) containing exactly the elements of the elements of \(x\). Since the formula uniquely specifies the elements of \(y\), we see that such a \(y\) is even unique. So let’s give it a name: \(\cup x \equiv y\). We call \(\cup x\) the <em>union</em> of \(x\).</p> <p>Other notations that are used sometimes include \(\cup_{s\in x} s\), and for two sets \(A\) and \(B\) we can take the union of their unordered pair, often denoted \(A \cup B \equiv \cup \{A,B\}\)</p> <hr/> <h4 id="axiom-5-power-set">Axiom 5: Power set</h4> <p>This assures the existence of a set that contains all subsets of a set, the so-called <em>power set</em>.</p> <blockquote> <p><em>For all</em> \(x\) <em>Exists</em> \(y\) <em>For all</em> \(z\) (\(z \in y\) <em>iff.</em> (<em>For all</em> \(s\) (\(s \in z\) <em>then</em> \(s \in x\))))</p> </blockquote> <p>We will define the relation symbol \(\subset\) with the following wff:</p> <blockquote> <p>\(x \subset y\) <em>iff.</em> (<em>For all</em> \(s\) (\(s \in x\) <em>then</em> \(s \in y\)))</p> </blockquote> <p>Then, the Power set axiom reads:</p> <blockquote> <p><em>For all</em> \(x\) <em>Exists</em> \(y\) <em>For all</em> \(z\) (\(z \in y\) <em>iff.</em> (\(z \subset y\)))</p> </blockquote> <p>The power set of \(X\) is unique by Extensionality; we denote it \(P(X)\).</p> <hr/> <h4 id="axiom-6-separation">Axiom 6: Separation</h4> <p>For every wff. \(\varphi\) which has the free variable \(x\), and all other variables in \(\phi\) are bound, we have the following axiom:</p> <blockquote> <p><em>Exists</em> y <em>For all</em> \(x\) (\(x \in y\) <em>iff.</em> \(\varphi\))</p> </blockquote> <p>Separation is not really one axiom if you consider it in first order logic. Rather, it is a whole family (i.e. informal set in our meta-language) which has an axiom for every instance of a wff. \(\varphi\). This is why it is often called an <em>axiom schema</em>: for every wff. \(\varphi\), we could say that we can substitute \(\varphi\) in the schema.</p> <p>So in fact, I was lying when I said that there are only 8 or 9 or so axioms for set theory: it is in fact an infinite theory, albeit countable in formal logic (since we can recursively enumerate every wff in our logical language).</p> <p>The set created by applying this axiom to a set \(X\) and a wff \(\varphi\) is often denoted \(\{x \in X: \varphi \}\).</p> <p>An important application of Separation is that we can define the <em>intersection</em> \(\cap x\) of a set \(x\), by taking the instance of this axiom schema for \(\varphi \equiv\) <em>For all</em> \(s \in P(X) \ x \in s\). This means that we can define, for arbitrary \(X\), a set of elements of elements of \(X\) such that its elements occur in <em>every</em> element of \(X\). This is our beloved \(\cap X\), or \(\cap_{x\in X} x\), or \(A\cap B \equiv \cap \{A,B\}\) for a pair of sets.</p> <p>You may have noticed that we have just introduced another shorthand notation, “<em>For all</em> \(a \in A*\)”. In general, we can define shorthand notation of quantification over a restricted domain \(A\) as:</p> <blockquote> <p>(<em>For all</em> \(a\in A \ \varphi\) ) <em>iff.</em> (<em>For all</em> \(a\) ( \(a \in A\) <em>then</em> \(\varphi\)))</p> </blockquote> <blockquote> <p>(<em>Exists</em> \(a\in A \ \varphi\) ) <em>iff.</em> (<em>Exists</em> \(a\) ( \(a \in A\) <em>and</em> \(\varphi\)))</p> </blockquote> <p>Another application of Separation is the construction of the <em>cartesian product</em> of two sets \(A\) and \(B\), denoted \(A \times B\) and defined as \(\{ x\in P(P(A\cup B))\) : <em>Exists</em> \(a\in A\) <em>Exists</em> \(b\in B \ x = \{\{a\},\{a,b\}\} \ \}\). In simpler terms it is exactly the set of all ordered pairs \((a,b)\) where \(a\in A\) and \(b\in B\).</p> <hr/> <p>With these first axioms and the definition of the cartesian product established, the next blog post will look into relations, especially <em>order relations</em>, <em>equivalence relations</em> and <em>functions</em>.</p>]]></content><author><name></name></author><category term="set-theory"/><summary type="html"><![CDATA[The general logical framework in which we will discuss sets, and the first axioms of set theory.]]></summary></entry><entry><title type="html">Logic, Models, Proofs</title><link href="https://matthijsmu.github.io/blog/2023/logic-model-theory/" rel="alternate" type="text/html" title="Logic, Models, Proofs"/><published>2023-08-04T00:00:00+00:00</published><updated>2023-08-04T00:00:00+00:00</updated><id>https://matthijsmu.github.io/blog/2023/logic-model-theory</id><content type="html" xml:base="https://matthijsmu.github.io/blog/2023/logic-model-theory/"><![CDATA[<h2 id="introduction-and-motivation">Introduction and Motivation</h2> <p>We will dive into formal first-order logic!</p> <p>The idea of mathematical logic is to make a formal language to express “logical sentences”, and set up rules to manipulate sentences in this language which we use to model our own everyday reasoning. Basically, we formalize reasoning and then reason about that system.</p> <p>Why formalize our reasoning? Two example reasons:</p> <ul> <li>First, to understand where our own mathematical reasoning can bring us. What <em>assumptions</em> (axioms) need to be part of a theory in order to derive certain <em>conclusions</em>? From a meta-level, what ideas can we <em>express</em> in first-order-logic? The theory of well-orders, for example, requires an axiom that states a property of subsets of a set (every subset of a well-ordered set has a least element). We will see that no equivalent statement exists in f.o.l., which is interesting: it means that we need a more expressible formal system to specify certain ideas.</li> <li>Second, we may want to automate reasoning and/or theorem-proving. To let computers do this for us, we need a formal framework to model the reasoning process. The programming language Prolog allows you to define <em>facts</em> as instances of <em>atoms</em> and <em>rules</em> as clauses of <em>atoms</em>. It bases computations on the set of its facts, the so-called <em>knowledge base</em>. As we will see, there is a very tight correspondence between Prolog’s atoms and f.o.l.’s <em>relation symbols</em>, rules and <em>wff</em>’s, the knowledge base and a <em>structure</em>, and finally between the <em>querying</em> of a Prolog program and the <em>interpretation</em> of a sentence in a model.</li> </ul> <hr/> <h2 id="defining-a-logical-language">Defining a logical language.</h2> <p>I will base the definitions on <a href="">Sets, Models and Proofs</a>, which was the book studied at my university. The book is super-rigorous and will start defining logical sentences in polish notation, because that uniquely fixes the structure of a sentence without the need of bracketing subclauses. Only then will they justify the bracketing notation, by tediously provjng the bijective correspondence between bracketed sentences and polish sentences.</p> <p>The book is in my opinion maybe a bit too rigorous, and I will skip this approach. You can probably justify the bracketed notation for yourself and this blog should not become more bureaucratic than a logic blog already is. Instead, I will define terms and wff’s in the way most logic textbooks approach it, that is, using some unspoken of brackets in the right place to enforce syntax.</p> <p>By the way, I assume you are familiar with some set theory. If not, consider <a href="/blog/2023/set-theory-1/">reading my series</a>!</p> <blockquote> <p><strong>Definition</strong> A <em>language</em> \(L\) is a triple \((\text{con}(L),\text{fun}(L),\text{rel}(L))\), where:</p> <ul> <li>\(\text{con}(L)\) is a set of <em>constants</em> or <em>constant symbols</em>.</li> <li>\(\text{fun}(L)\) is a set of <em>function symbols</em>.</li> <li>\(\text{rel}(L)\) is a set of <em>relation symbols</em>. Every relation or function symbol comes with an <em>arity</em>, which is</li> </ul> </blockquote> <p>Practically, we could do without constants and use function symbols with arity 0 instead. However, the distinction (or, different naming) is often useful in proofs that have differing cases for terms that are constants and terms that are functions applied to terms.</p> <p>As an example, we could look at the language \(L_{group}\) that has one constant, \(e\) for the identity, one function symbol \(\circ\) for group multiplication (we will write it infix, don’t worry), and no relation symbols.</p> <p>Additionally, we will use other symbols. We assume that all symbols are distinct, so that they are unambiguous to recognize in terms and formulas. These additional symbols are:</p> <ul> <li><em>Variables</em>, which is some countably infinite set of symbols</li> <li>The <em>equality symbol</em> \(=\)</li> <li>The <em>falsum</em> or <em>absurdity</em> symbol \(\bot\)</li> <li>Logical <em>connectives</em>, which are \(\land,\lor,\lnot,\rightarrow\)</li> <li><em>Quantifiers</em>, which are in f.o.l. the <em>universal</em> quantifier \(\forall\) and the <em>existential</em> quantifier \(\exists\)</li> </ul> <blockquote> <p><strong>Definition</strong> We denote the union of the set of \(L\)’s symbols, the variables, the equality symbol, falsum, the connectives and the quantifiers, \(\mathcal{C}_L\).</p> </blockquote> <blockquote> <p><strong>Definition</strong> (Kleene star notation) We denote the set of all finite strings over some alphabet \(\mathcal{A}\), \(\mathcal{A}^* = \cup_{n=0}^\infty \mathcal{A}^n\) .</p> </blockquote> <blockquote> <p><strong>Definition</strong> The set of \(L\)-terms of a language \(L\) is the smallest subset \(T\) of \(\mathcal{C}^*_L\) such that:</p> <ol> <li>If \(c \in \text{con}(L)\) then \(c \in T\)</li> <li>If \(x\) is a variable then \(x \in T\)</li> <li>If \(t_1, ... , t_n \in T\) and \(f \in \text{fun}(L)\) with arity \(n\), then \(f(t_1,...,t_n) \in T\)</li> </ol> </blockquote> <p>I want to note two things:</p> <ul> <li> <p>The above objects are all (purely syntactical) strings of symbols! This is why the bracketed notation is a tad bit non-rigorous, because for example what is the exact length of such a term if we need to know? And where do the bracket symbols \((\) come from \()\) in the first place? They are not in the alphabet!</p> </li> <li> <p>Next, why is there a “smallest subset”? The definition means “smallest” in the sense that any set that satisfies 1., 2. and 3., must contain \(T\) as a subset. Now why would such a set exist? Think about it in this way: if \(\{T_\alpha\}_{\alpha\in I}\) is some collection of sets that all satisfy 1. and 2. and 3., then \(T = \cap_{\alpha\in I}T_\alpha\) will also contain all variables and constants, and since terms that are in \(T\) are in \(T_\alpha\) for all \(\alpha\in T\), concatenation with a function symbol will keep them in all \(T_\alpha\) and hence in \(T\). That is why there must be a minimum set: if not, we take the intersection of two different minimal sets and reach a contradiction!</p> </li> </ul> <blockquote> <p><strong>Definition</strong> (Substitution on \(L\)-terms) For \(t, s \in T\) and \(x\) a variable or a constant of \(L\). We define \(t[s/x]\), the <em>substitution of</em> \(s\) <em>for</em> \(x\) <em>in</em> \(t\), as:</p> <ul> <li>if \(t = f(t_1, ..., t_n)\) for some \(n\)-ary function symbol \(f\) and terms \(t_1, ... , t_n \in T\), then $$t[s/x] = f(t_1[s/x], … , t_n[s/x]).</li> <li>if \(t\) is a constant or a variable, then if \(t = x\) we have \(t[s/x] = s\) and otherwise \(t[s/x] = t\)</li> </ul> </blockquote> <p>We can prove by <em>structural</em> induction on terms that this immediately implies that \(t[s/x]\) is again in \(T\) for \(t,s\in T\) and \(x\) a constant or variable. The principle of structural induction means that we have to prove the statement for all constants and variables \(t\) and also that the statement holds for \(f(t_1, ... t_n) \in T\) for all \(f\in \text{fun}(L)\) \(n\)-ary, if it already holds for \(t_1, ... t_n\).</p> <hr/> <h2 id="well-formed-formulas">Well-formed formulas.</h2> <p>For formulas, we give a very similar definition that makes use of terms.</p> <blockquote> <p><strong>Definition</strong> The set of formulas \(F\) of a language \(L\) is the smallest subset of \(\mathcal{C}_L^*\) satisfying:</p> <ul> <li>It contains all <em>atomic formulas</em>, which are strings of the form: <ol> <li>\(t = s\) for \(s, t \in T\).</li> <li>\(R(t_1, ..., t_n)\) for \(R \in \text{rel}(L)\) an \(n\)-ary relation symbol and \(t_1,...,t_n\in T\).</li> <li>\(\bot\).</li> </ol> </li> <li>It contains all strings of the form: <ol> <li>\(\varphi \land \psi\), \(\varphi \lor \psi\), \(\varphi \rightarrow \psi\), \(\lnot \varphi\), for \(\varphi, \psi \in F\).</li> <li>\(\forall x \varphi\), \(\exists x \varphi\) for \(x\) a variable, \(\psi \in F\).</li> </ol> </li> </ul> </blockquote> <p>The reason that this is well-defined is because again, when a collection of sets satisfying 1. to 5. is intersected, this intersection satisfies 1. to 5. as well.</p> <p>This definition enables us to have an induction principle on \(F\) as well. That is:</p> <blockquote> <p><strong>Induction Principle on Formulas</strong> If some statement holds for all atomic formulas, and if we have that (if the statement holds for \(\varphi \in F\) and \(\psi \in F\), then it holds for \(\varphi \land \psi\), \(\varphi \lor \psi\), \(\varphi \rightarrow \psi\), \(\lnot \varphi\), \(\forall x \varphi\) and for \(\exists x \varphi\) for \(x\)), then the statement holds for all \(L\)-formulas. <strong>Proof</strong> Let \(E\) be the set of \(L\)-formulas for which the statement holds. Since it is given that \(E\) satisfies 1. to 5., it follows by minimality of \(F\) that \(F\subset E\), so we are done.</p> </blockquote> <p>With this induction principle, it is now simple to prove a recursion principle:</p> <blockquote> <p><strong>Recursion Principle on Formulas</strong> Let \(V\) be the set of variables, \(A\) the set of atomic formulas and \(X\) some arbitrary set, with functions: \(f_a:A \rightarrow X\) \(f_\land, f_\lor, f_\rightarrow: X\times X\rightarrow X\) \(f_\lnot: X \rightarrow X\) \(f_\forall, f_\exists: V\times X \rightarrow X\) Then there is a unique function \(f: F\rightarrow X\) that satisfies: \(f(\varphi) = f_a(\varphi)\) if \(\varphi\) is atomic \(f(\varphi) = f_\land(f(\chi), f(\psi))\) if \(\varphi\) is \(\chi \land \psi\) \(f(\varphi) = f_\lor(f(\chi), f(\psi))\) if \(\varphi\) is \(\chi \lor \psi\) \(f(\varphi) = f_\rightarrow(f(\chi), f(\psi))\) if \(\varphi\) is \(\chi \rightarrow \psi\) \(f(\varphi) = f_\lnot(f(\psi))\) if \(\varphi\) is \(\lnot \psi\) \(f(\varphi) = f_\forall(x, f(\psi))\) if \(\varphi\) is \(\forall x \psi\) \(f(\varphi) = f_\exists(x, f(\psi))\) if \(\varphi\) is \(\exists x \psi\)</p> </blockquote> <p>Notee: try not to confuse (meta) equalities with semtences using the <em>equality symbol</em>.</p> <blockquote> <p><strong>Proof</strong> (Sketch)</p> <ul> <li>Unicity: let \(f,g\) both satisfy the recursion and let \(\psi\) be the <em>shortest</em> formula for which \(f(\psi) \not = g(\psi)\). If \(\psi\) is atomic, we see that this is not possible because \(f,g\) are both fixed as \(f_a\) on atomic formulas. So \(\psi\) is composite. We can exhaust all cases and in every case we have to conclude that not all shorter subformula of \(\psi\) can have the exact same image under \(f\) as under \(g\), hence \(\psi\) is not shortest and we get a contradiction.</li> </ul> </blockquote> <p>This means we can now safely define functions by recursion on formulas. The principle example of this is of course substitution:</p> <blockquote> <p><strong>Substitution on \(L\)-formulas</strong> For \(\varphi \in F\), \(x\in V\) and \(s\in T\), we define \(\varphi[s/x]\) by recursion:</p> <ul> <li>\(\varphi[s/x] = (t_1[s/x]=t_2[s/x])\) if \(\varphi = (t_1=t_2)\) and likewise we just substitute on terms for other atomic formulas.</li> <li>\((\psi _ \chi)[s/x] = (\psi[s/x] _ \chi[s/x])\) for \(_ \in \{\land,\lor,\rightarrow\}\)</li> <li>$$(\lnot \psi)[s/x] = \lnot(\psi[s/x])</li> <li>\((\forall y \psi)[s/x] = \forall y (\psi[s/x])\) if \(y \not = x\), otherwise \((\forall x \psi)[s/x] = \forall x \psi\)</li> <li>\((\exists y \psi)[s/x] = \exists y (\psi[s/x])\) if \(y \not = x\), otherwise \((\exists x \psi)[s/x] = \exists x \psi\)</li> </ul> </blockquote> <p>The above definition is interesting: rather than defining what a <em>bound</em> variable is and explaining why bound variables should not be substituted, we first explicitly state substitution rules, and from this we can define what it means for a variable to <em>occur</em> in a formula and to be <em>bounded</em>. A variable is bounded exactly when it is <em>not replaced by substitution</em>!</p> <blockquote> <p><strong>Definition</strong> (occurrence, bounded, free, closed) Let \(\varphi \in F\).</p> <ul> <li>An <em>occurrence</em> of \(x\in V\) in \(\varphi\) is a natural number \(i\) such that the \(i-th\) elemnt of the string \(\varphi\) is \(x\).</li> <li>If \(i\) is an occurrence of \(x\) in \(\varphi\), and let \(u\in V\) not occur in \(\varphi\). Then the occurrence \(i\) of \(x\) is called <em>bound</em> if the \(i\)-th element of \(\varphi[u/x]\) is \(x\).</li> <li><em>free</em> occurrences are those occurrences that are not bound.</li> <li>A formula that has no free occurrences of any variable \(x\in V\) is called <em>closed</em>, or also a <em>sentence</em>.</li> </ul> </blockquote> <h2 id="legitimacy-of-substitutions">Legitimacy of substitutions</h2> <p>Next, I will point out something that will immediately make you think that there is a hiat in our definitions so far. Suppose I give you the formula \(\varphi = \forall x R(x,y)\). It expresses a property for \(y\), you could say. It should mean the same as \(\forall x R(x,y)\). Yet if we substitute the term \(f(x,z)\) for \(y\), we get two formulas that seem to have wildly different semantics:</p> <blockquote> <p>\(\forall x R(x, f(x,z))\) versus \(\forall u R(u, f(x,z))\)</p> </blockquote> <p>This cannot be right. The problem is precisely that occurences of some variables (namely \(x\)) in the substituted term \(t\) get bound in \(\varphi[t/y]\). Otherwise, this bifurcation of semantics would not happen. Hence, we make this type of substitution <em>non-legitimate</em>. Closed terms (no free variables in the term, that is) are of course always legitimate to substitute.</p> <h2 id="structures-and-interpretation-defining-truth">Structures and interpretation: defining truth</h2> <p>So far, we have not yet spoken of the semantics or meaning of f.o.l.. Only in the previous section, I alluded to it in order to motivate legitimacy of substitutions, but I could have also thrown it at you without any motivation, and if you were naive and didn’t have an idea of what the logical connectives etcetera <em>meant</em>, we might as well have used completely different symbols with no apparent semantics such as tables, chairs and beer mugs, and once we have developed a formal system to manipulate these syntactically it would make no difference as to which relationships between the names we would be able to derive.</p> <p>But we want to do logic because we want to understand our reasoning, and we want to be sure that the abstract systems defined to study it, actually matches our interpretation (semantics) or “understanding” of logic, whatever that means. As mathematicians, we value a rigorous definition of this “interpretation” and these “semantics” as well, so we develop them using <em>model theory</em>.</p> <p>The basic idea is that we take, on our meta-level from which we study the formal wff’s, a meta-level set with on it \(n\)-ary functions and relations (not the symbols, but actual functions and relations from set theory). These we call the <em>interpretations</em> of the function and relation <em>symbols</em> in the logical sentences. We then recursively define what it means to interpret <em>terms</em>, then <em>closed formulas</em> and from this we derive a definition of truth. The development of this theory is largely attributed to Tarski. Let’s begin.</p> <hr/> <blockquote> <p><strong>\(L\)-structure</strong> an \(L\)-<em>structure</em> \(M\) is a <em>nonempty</em> set \(M\) together with:</p> <ul> <li>for every constant \(c \in \text{con}(L)\) an element \(c^M\in M\).</li> <li>for every \(n\) for every \(n\)-ary function symbol \(f\in\text{fun}(L)\) a function \(f^M:M^n \rightarrow M\).</li> <li>for every \(n\) for every \(n\)-ary relation symbol \(R\in \text{rel}(L)\) a subset \(R\subset M^n\)</li> </ul> </blockquote> <p>We call \(c^M\) the interpretation of \(c\) in \(M\), and the same goes for \(f^M\) and \(R^M\). first, we define the language \(L_M\) for \(L\) a language and \(M\) an \(L\)-structure to be the language that has the same function symbols and relation symbols of \(L\), but now as its constants it has \(\text{con}(L_M) = \text{con}(L)\cup M\). As an example, we can take \(L_{ring}\), the language of rings, and consider the \(L\)-structure \(M = \mathbb{Z}_3 = \{\bar0, \bar1\, \bar3\}\). Then \(\text{con}(L_M) = \{0,1,\bar0,\bar1,\bar3\}\).</p> <p>Note that if we define \(m^M = m\in M\) for \(m\in \text{con}(L_M)\), then \(M\) is also an \(L_M\)-structure and this is how we will interpret \(L_M\)-constants in \(M\) always. This allows us to recursively define interpretations of <em>closed</em> \(L_M\)-terms:</p> <blockquote> <p><strong>Interpretation of closed terms</strong> For \(t\) an \(L_M\)-term we define the interpretation \(t^M\in M\) as:</p> <ul> <li>for \(t = f(t_1,...,t_n)\), we set \(t^M = f^M(t_1^M, ..., t_n^M)\)</li> <li>for \(t\) a constant \(c\in \text{con}(L)\), we set \(t^M = c^M\)</li> </ul> </blockquote> <p>This is well-defined and unique by structural recursion on terms. Also, note that \(t^M\in M\) can be proved by recursion on terms as well. Finally, I want to remark that there is really not much we can do to interpret <em>open</em> terms. How should we interpret an arbitrary variable anyway? Such an interpretation can for example not depend on the free variable \(x\) itself, because we want \(t[u/x]\) and \(t\) to have the same semantics, for \(u\) another variable that is not yet in \(t\).</p> <p>Next comes “Tarski’s definition of truth”. We write \(M\models\varphi\) and say that \(M\) <em>satisfies</em> \(\varphi\), \(\varphi\) <em>holds in</em> \(M\) or also that \(\varphi\) <em>is true</em> in \(M\). If not \(M\models \varphi\), that is \(M\) does not satisfy \(\varphi\), then we write \(M\not\models\varphi\).</p> <blockquote> <p><strong>Interpretation of closed formulas</strong> For a closed \(L_M\)-formula \(\varphi\) we define the relation \(M\models \varphi\) as:</p> <ul> <li>If \(\varphi\) is a closed atomic formula, it is either \(\bot\), \((t_1=t_2)\) or \(R(t_1,...,t_n)\) for \(t_1, ... , t_n\) closed \(L_M\)-terms. So define: <ul> <li>\(M\models\bot\) never holds.</li> <li>\(M\models (t_1= t_2) iff.\)t_1^M = t_2^M\(in\)M$$.</li> <li>\(M\models R(t_1, ... , t_n)\) iff \((t_1^M, ... , t_n^M )\in R^M\)</li> </ul> </li> <li>If \(\varphi\) is not atomic, it is either a connective followed by one or two closed \(L_M\)-formulas or it is one of the two quantors followed by a variable \(x\) and a \(L_M\)-formula that may have only \(x\) as a free variable but does otherwise not have any other variables with free occurences. Hence define: <ul> <li>\(M\models(\chi\land\psi)\) iff \(M\models\chi\) and \(M\models\psi\)</li> <li>\(M\models(\chi\lor\psi)\) iff \(M\models\chi\) or \(M\models\psi\)</li> <li>\(M\models(\chi\rightarrow\psi)\) iff \(M\models\psi\) whenever \(M\models\chi\)</li> <li>\(M\models(\lnot\psi)\) iff not \(M\models\psi\), hence iff \(M\not\models\psi\).</li> <li>\(M\models\forall x \psi\) iff \(M\models\psi[m/x]\) for all \(m\in M\)</li> <li>\(M\models\exists x \psi\) iff \(M\models[m/x]\) for any \(m\in M\)</li> </ul> </li> </ul> </blockquote> <blockquote> <p><strong>Validity and logical equivalence</strong></p> <ul> <li>An \(L\)-formula \(\varphi\) we call <em>valid</em> if for every \(L\)-structure \(M\) and every substitution of constants of \(M\) for free variables in \(\varphi\) we have \(M\models \varphi\).</li> <li>Two \(L\)-formulas \(\chi\) and \(\psi\) are called equivalent if the formula \(\chi\leftrightarrow\psi\) is valid.</li> </ul> </blockquote> <p>Here we have the shorthand notation \(\chi\leftrightarrow\psi \equiv (\chi\leftarrow\psi) \land (\chi\rightarrow\psi)\)</p> <p>One can then very tediously and bureaucratically prove all the logical equivalences of formulas that we already know very well intuitively, and I will not do that here.</p> <p>An \(L\)-theory \(T\) is simply a set of closed \(L\)-formulas. \(M\) is called a <em>model</em> for \(T\) if it is an \(L\)-structure that satisfies all formulas in \(T\). We call \(T\) <em>consistent</em> if it has (“admits”) a model.</p> <p>An interesting theorem says that \(T\) is consistent iff. every one of its finite subtheories is consistent. This is the so-called <em>compactness theorem</em>, which can indeed be proven via the compactness theorem from topology. Another way of proving it is with <em>ultrafilters</em>. A third way is to just use proof theory and</p> <hr/> <h2 id="well-orders-and-the-limitations-of-first-order-logic">Well-orders and the limitations of first-order logic</h2> <p>A well-order is a set \(W\) with an order relation \(\leq\) on it such that for every nonempty subset \(S\subset W\), \(S\) has a <em>least</em> element \(l\in S\), i.e. for all \(s\in S\) we have \(l\leq s\). Writing this theory in f.o.l. requires us to make a statement about all <em>subsets</em> of \(W\) rather than all <em>elements</em> of \(W\). This is nontrivial, because in the interpretation semantics we described above, we <em>quantify over the elements of a model</em>. So can it be done? The answer is no, and the proof of it makes use of the compactness theorem.</p> <h2 id="proofs">Proofs</h2> <p>We say that \(T\models \varphi\) for \(T\) a \(L\)-theory and \(\varphi\) a \(L\)-sentence, if for every model \(M\) that satisfies \(T\)’s axioms, we have that \(M\models \varphi\).</p> <p>Many frameworks are used to <em>formally</em> prove logical sentences. For such a framework to be acceptable, we want it to be able to be able to generate a formal proof from \(T\) for all \(\varphi\) such that \(T\models\varphi\) (completeness), and vice versa we want it to not be stronger than that, that is, if \(T\proves \varphi\) (this is the notation we will use for “there exists a proof for \(\varphi\) from assumptions in \(T\)”) then \(T\models\varphi\) (soundness).</p> <p>The set of proofs is defined as the smallest subset of the set of all <em>marked trees</em> over \(F\), that satisfies certain closure properties. I will now explain what marked trees are in this context (if you study cs/discrete maths, you are probably aware that there are 1001 definitions for trees and all sorts of variations) and what the closure properties are.</p> <blockquote> <p><strong>Tree</strong> A (rooted) tree over a set \(X\) is a finite subset \(Y\subset X\) that is:</p> <ul> <li>partially ordered,</li> <li>and has a least element,</li> <li>and for any two \(x,y\in Y\) then \(\{x,y\}\) has an upper bound iff. \(x\leq y\) or \(y\leq x\).</li> </ul> </blockquote> <p>We can display a tree as its Hasse diagram. You can then see that the third rule assures that there is no way two “upward branches” can meet again when we go up the diagram, because</p> <p>The definition given in <em>Sets, Models and Proofs</em> is somewhat different, though I believe that because \(Y\) is finite, you might be able to prove that the two definitions are equivalent:</p> <p><strong>Tree (Sets, Models, Proofs)</strong> A tree over a set \(X\) is a finite subset \(Y\subset X\) that is:</p> <blockquote> <ul> <li>partially ordered,</li> <li>for any \(x\in Y\) the set \(\downarrow (x) = \{y\in Y:y\leq x\}\) is well-ordered.</li> </ul> </blockquote> <p>The <em>maximal elements</em> \(L\subset Y\) of the tree are called <em>leaves</em>, they will act as our assumptions. Important is that assumptions can be <em>marked</em>, so we need to extend our trees to include a <em>marking function</em>.</p> <blockquote> <p><strong>Marked Tree</strong> is a tree with a function \(f:L\rightarrow \{0,1\}\) or equivalently a set \(L_{marked}\subset L\).</p> </blockquote> <p>It is very difficult to get a picture of these abstract definitions in your head. So let me include a picture of a marked tree over \(F\), some set of abstract formulas. My example also happens to be a proof tree, but I will get back to that later.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prooftree-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="proof tree image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the example, the bottom layer is the root, and the partial order relation is precisely displayed by having two formulas \(\chi\) and \(\psi\) such that \(\chi &lt; \psi\) and that there is no such \(\xi\) that \(\chi &lt; \xi &lt; \psi\) put in the diagram in such a way that \(\psi\) is on a level immediately above \(\chi\) and reachable from \(\chi\) in one step up the tree. Marked leaves are shown as elements [between rectangular brackets]. You should ignore the numbering for now, it is not part of the tree definition in any way.</p> <blockquote> <p><strong>Proof Tree</strong> Given a logical language \(L\), the set \(\mathcal T\) of proof trees is the smallest set of marked trees over the set of \(L\)-formulas that satisfies the following closure properties:</p> </blockquote> <ul> <li>Actually, you can look up the definition in the freely downloadable book <a href="https://www.a-eskwadraat.nl/Onderwijs/Boekweb/Artikel/48/Dictaat/Downloaden"><em>Sets, Models and Proofs</em></a>. I don’t want to literally copy their definition here, that feels too much like plagiarism.</li> <li>The general idea is that every connective and every quantor has an introduction rule to introduce it into a formula, and an elimination rule to eliminate it from a formula. Then there are assumption trees and finally a valid proof is a proof tree where all assumptions (leaves) except maybe those that occur in the <em>assumed</em> theory \(T\) and/or the formula \(\exists x (x=x)\) have been marked.</li> </ul> <p>Once you have read the above definition in the provided book, you can check for yourself that the example tree I provided earlier is indeed a proof tree. It is a proof, or better, a proof schema (since it is more of a template where \(\varphi\) is a completely abstract formula) of the <em>law of excluded third</em>. Any proof for this law makes use of \(\lnot E\), and that is also why in <em>Intuitionistic Logic</em>, where \(\lnot E\) is not one of the closure laws for proof trees, the law of excluded third does not hold.</p>]]></content><author><name></name></author><category term="logic"/><summary type="html"><![CDATA[Something about logic and models for logics (Unfinished)]]></summary></entry><entry><title type="html">Set Theory, Numbers and Logic</title><link href="https://matthijsmu.github.io/blog/2023/set-theory-numbers-logic/" rel="alternate" type="text/html" title="Set Theory, Numbers and Logic"/><published>2023-08-04T00:00:00+00:00</published><updated>2023-08-04T00:00:00+00:00</updated><id>https://matthijsmu.github.io/blog/2023/set-theory-numbers-logic</id><content type="html" xml:base="https://matthijsmu.github.io/blog/2023/set-theory-numbers-logic/"><![CDATA[<p><strong>Update: I have decided to discontinue the series on Set Theory, Numbers and Logic. In particular, I left points 5-11 of the below outline unfinished. The reason for this is that I am currently following further advanced courses and would like to continue blogging about those. Right now, I feel that the rigorous approach that I have been trying to enforce while writing my blogs has deprived them from interest. I would like to write things more summarizingly sometimes, but that makes me wonder what the actual goal of these posts is in the first place. I did not want to develop a 6 EC course within a blog series, but the material that I originally planned to discuss is taught during two 6 EC courses at my University, and only now I realize that this also means a blog on the subjects would become very lengthy.</strong></p> <p><strong>I think that developing all theory step-by-step such as taught in a course may be a better fit for textbooks than for blog posts anyway. I want to keep you, the reader, interested and excited about mathematics and I feel that the way I have been writing blog posts so far is a very different approach: the blogs are long, wordy and bureaucratic. Which is perfect if you dive into a course with zero knowledge on the subject and want to learn everything step-by-step. But this has never been the aim of my blog, honestly.</strong></p> <h3 id="an-overview-of-the-blogs-on-set-theory-numbers-and-logic">An Overview of the blogs on Set Theory, Numbers and Logic</h3> <p>As a first subject to write about in my blog, I chose to do a series on set theory, logic and models for number systems. These are based on courses I followed at Radboud University during my pre-university studies (Introduction to Mathematics and Numbers) and first year (though Mathematical Logic is actually a 2nd year course). My idea is to go from sets to numbers to logic and then to sets again. In particular:</p> <ol> <li> <p>We start with the basic axioms of set theory, which are formulated in natural language. We need in particular <em>Extensionality</em>, <em>Empty set</em>, <em>Pairing</em>, <em>Union</em>, <em>Power set</em> and <em>Separation</em>, to be able to construct the <em>Cartesian product</em> of two sets.</p> </li> <li> <p>This is followed by the intruction of <em>relations</em> as subsets of the Cartesian product. We will consider pre-orders, partial and total orders, equivalences, partitions.</p> </li> <li> <p>Next are <em>functions</em>, which are in fact also a relation. We discuss composition, injections, surjections and bijections. We also introduce the axiom called <em>Replacement</em>, which is a natural thing to do in the light of functions. I will also explain in detail why we need it.</p> </li> <li> <p>We then move on to some classical results in set theory, such as the <em>Schröder-Cantor-Bernstein theorem</em>, and Cantor’s diagonal argument. I <em>may</em> introduce <em>Cardinal numbers</em>, which are a way of expressing theorems about the sizes of sets. It depends on whether I would like to first develop some examples on various infinite sets, which will likely be done in a much later post.</p> </li> <li> <p>Fifth, we introduce yet two more axioms: the <em>Foundation axiom</em> and the <em>axiom of Infinity</em>. We use these to construct a <em>model</em> for the natural numbers. What exactly a <em>model</em> is, I shall discuss much later. We prove some theorems on the natural numbers and derive a <em>principle of induction</em>, which is actually also one of the <em>axioms</em> in the theory of the natural numbers (PA).</p> </li> <li> <p>I would like to delay the introduction of the <em>Axiom of Choice</em> (AC) for as long as possible, but I think the reader needs to be aware of its existence to understand that care needs to be taken when reasoning about infinite sets and functions. It is easy to think every surjection has a section, and when working with infinities one might just apply AC without even noticing. The axiom however, has dramatic consequences, which I shall discuss in the same blog.</p> </li> <li> <p>We next discuss the theory of well-ordered sets and some variations on the principle of induction for w.o.’s and principles of recursion on w.o.’s.</p> </li> <li> <p>Finally, we prove <em>Hartogs’s lemma</em>, which is a result on w.o.’s that does not rely on AC. We can use Hartogs’s lemma to show equivalence of AC with Zorn’s lemma, Principle of Cardinal Comparability (also known as <em>Trichotomy</em>) and the <em>Well-Ordering Theorem</em> (also known as <em>Zermelo’s Theorem</em>). This is all inspired on the proof given in <a href="https://link.springer.com/book/10.1007/978-3-319-92414-4"><em>Sets, Models and Proofs</em></a>, chapter 1.5.</p> </li> <li> <p>I want to continue with a quick sketch of how to construct the integers, then the rational, then the real numbers from the natural numbers. Garling’s <a href="https://www.cambridge.org/core/books/course-in-mathematical-analysis/C0D89CA72FF3ED2B7F3280A922CF9D5B">A Course in Mathematical Analysis</a> devotes an entire chapter to this, so this may take much longer than expected. At that moment, I may just want to move on and end the blog, so I won’t promise anything yet.</p> </li> <li> <p>We will go over the definition of <em>wff.</em>’s in a formal logical language \(L\), and define theories, \(L\)-structures, interpretations and models for theories. I think examples are particularly important, to show that a theory in general has a multitude of models.</p> </li> <li> <p>I will skip much of the bureaucracy that a 2nd year student taking Mathematical Logic has to go to, because I want to keep my readers. Probably, I will jump straight to Gentzen-style natural-deduction-proof-trees and formally prove a few theorems. I will definitely not discuss soundness or completeness of natural deduction, but maybe I will give a sketch.</p> </li> </ol> <p>These blogs are intended to be read in an approximately linear fashion, starting from Set Theory, through Number Theory to Logic. I based these blogs on the following material:</p> <ul> <li> <p>Set Theory: based on lecture notes of Klaas Landsman, <a href="https://www.math.ru.nl/~landsman/InleidingWiskunde2017.pdf"><em>Introduction to Mathematics</em></a>, as well as Garling’s <a href="https://www.cambridge.org/core/books/course-in-mathematical-analysis/C0D89CA72FF3ED2B7F3280A922CF9D5B"><em>A Course in Mathematical Analysis, Volume I</em></a>, Part 1, Chapter 1.</p> </li> <li> <p>Numbers: based on <em>Introduction to Mathematics</em> and Garling’s A Course in Mathematical Analysis, Volume I, Part1, Chapter 2.</p> </li> <li> <p>Logic: based on <a href="https://link.springer.com/book/10.1007/978-3-319-92414-4"><em>Sets, Models and Proofs</em></a>.</p> </li> </ul> <p>These books were used in the first-semester course Introduction to Mathematics and the 4th-semester course Logic taught at Radboud University. They inspired me to write a blog, because the rigorous development of sets, functions, number systems, etcetera, is in many ways highly nontrivial and subtle, especially when dealing with infinite sets. The Logic blog is in some sense a natural continuation of rigorous set theory, but can also be read as a separate blog on formal logic (i.e. proofs and model theory). It does however also require an informal development of sets to start with, and the number systems discussed in Numbers will certainly provide us with some interesting models for various theories.</p> <hr/> <h4 id="sets">Sets</h4> <p>This blog starts with an axiomatic definition of sets. It will then continue to explore set theory, diving into relations, including orders, equivalences and functions. Once functions have entered the play, we introduce the axiom of choice and various equivalent axioms, and study their equivalence using the theory of well-orders.</p> <p>When defining sets, the aim is not to explain “what” a set is, but rather to specify the axioms according to which sets behave. The notion of a set is simply too abstract to describe intuitively, moreover if we want to do rigorous mathematics with them then the last thing we want is only an intuitive notion: we need the laws according to which the object behaves to make rigorous derivations of other properties.</p> <hr/> <h4 id="logic">Logic</h4> <p>This separation between intuition and formalism will be taken a step further when we will discuss logic. In first order logic, we study theories, which are sets of logical sentences, another mathematical object, in two ways: one way is to interpret the sentences using a model, that is, we define what it means for a sentence to hold in a structure and if a structure satisfies all sentences of the theory, it “models” the sentence. This is a good way to create an intuition of the objects that are described by the theory. The other way of studying theories looks not at interpretations of sentences but at rules for recombining logical sentences into new logical sentences. The recombination leads us to definitions such as proof trees and ultimately to the notion of a formal “proof” of a “theorem”.</p> <p>If you have not seen mathematical logic yet, be aware that these proofs are different from a proof using natural language that is usual in mathematics texts. Formal proofs in logic are mathematical objects that we can again prove properties/theorems about in natural language. In a sense, we study the way we prove and reason about mathematical things from a meta-level.</p> <hr/> <h4 id="numbers">Numbers</h4> <p>Even though logic studies mathematical reasoning at the lowest possible level of abstraction, a rigorous course in model theory and proof trees is usually only given in the second year of a mathematics B.S. The reason is that students should be familiar with theories such as those of the natural, integer, rational and real numbers before being exposed to formal theories and models for them. These number systems are the focus of Numbers. I highly recommend reading this blog secondly, before continuing to Logic. The real number system will also provide the foundations for Mathematical Analysis (which is, not coincidentally, the actual subject of Garling’s book), which I might discuss in a later blog, although I think I will by then prefer to tell you something about more advanced material.</p> <hr/> <h4 id="further-studies">Further Studies</h4> <p>The theory of rings, groups, fields and ordered fields in these posts is only developed for the integer, rational and real number systems, Much more can be said about these theories in, say, the courses at Radboud such as Group Theory and Rings and Fields. It is unfortunate. If you do like to know, you should consider studying mathematics (advertising intended)!</p>]]></content><author><name></name></author><category term="set-theory"/><summary type="html"><![CDATA[An overview of my first blog series, which explains set theory, numbers and some mathematical logic.]]></summary></entry></feed>